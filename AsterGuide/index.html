<!DOCTYPE html>
<head>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
<script src="http://d3js.org/d3.v3.min.js"></script>
<script>
    var data;
    $(document).ready(function(){
        $.getJSON('aster_functions.json', function(response){
            data = response;
        });
    });
    /*
    names = [];
    for (i=0; i < mydata['children'].length; i++){
        for (j=0; j < mydata['children'][i]['children'].length; j++){
            names.push(mydata['children'][i]['children'][j].name);
        }
    };
    */
   
</script>


<style>

    h4 {
        color:#173E6E;
    }
    h2 {
        color:#173E6E;
    }

    body {
        background-image:url("noisy-texture.png");
    }
    .node {
      cursor: pointer;
    }
    .node circle {
      fill: #FF6E00;
      stroke: #FF6E00;
      stroke-width: 1.5px;
    }
    .node text {
      font: 11px sans-serif;
    }
    .link {
      fill: none;
      stroke: #ccc;
      stroke-width: 1.5px;
    }
    .text_scroll {
        max-height:500px;
        overflow-y:scroll;
    }
</style>

</head>
<body>
    <div class="row">
        <div class="col-md-12">
            <h2 class="text-center"><span style="color:#FF6E00">Teradata Aster</span> Functions Guide</h2>
        </div>
        <div class="col-md-6" id="chart" style="padding-right: 0px;"></div>
        <div class="col-md-6" id="function_text" style="padding-left: 0px;">
            <div id="Attribution" style="display:none">
                <h2 class="text-center">Attribution</h2>
                <h4>Summary</h4>
                <p>
                    The attribution operator is often used in web page analysis. Companies would like to assign weights to pages before certain events, such as a 'click' or a 'buy'. This attribution function enables you to calculate attributions by using a wide range of distribution models.</p>
                <p>
                    There are two different ways to call the attribution SQL-MR function, depending on how many inputs you are using. The multiple input and the single input function calls use very different syntax, so they are documented separately:
                </p>
                <h4>Usage</h4>
                <ul class="nav nav-tabs" role="tablist">
                    <li class="active"><a href="#multiple_attribution" role="tab" data-toggle="tab">Multiple Input</a></li>
                    <li><a href="#single_attribution" role="tab" data-toggle="tab">Single Input</a></li>
                </ul>
                <div class="tab-content">
                    <div class="tab-pane active" id="multiple_attribution">
                        <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
                    </div>
                    <div class="tab-pane" id="single_attribution">
                        <script src="https://gist.github.com/PMeinshausen/f5db870c53700d8217a1.js"></script>
                    </div>
                </div>
            </div>
            <div id="CMAVG" style="display:none">
                <h2 class="text-center">CMAVG</h2>
                <h4>Summary</h4>
                <p>
                    The Cumulative Moving Average (CMAVG) function computes the average of a value from the beginning of a series.
                </p>
                <h4>Background</h4>
                <p>
                    In a cumulative moving average, the data are added to the dataset in an ordered data stream over time. The objective is to compute the average of all the data at each point in time when new data arrived. For example, an investor may want to find the average price of all of the stock transactions for a particular stock over time, up until the current time.
                </p>
                <p>
                    The cumulative moving average computes the arithmetic average of all the rows from the beginning of the series:
                </p>
                <p style="font-family:Latin Modern Mono">
                    new_smavg = sum(a1 ... aN) / N
                </p>
                <p>
                    where N is the number of rows from the beginning of the dataset.
                </p>
                <h4>Usage</h4>
                <script src="https://gist.github.com/PMeinshausen/d7c13e8f7f301a569614.js"></script>
            </div>
            <div id="DTW" style="display:none">
                <h2 class="text-center">DTW</h2>
                <h4>Background</h4>
                <p>
                    Dynamic time warping (DTW) is an algorithm that measures the similarity between two sequences that vary in time or speed.
                </p>
                <p>
                    For example, DTW can detect similarities in walking patterns, even if in one video the person is walking slowly and in another, the same person is walking fast.
                </p>
                <p>
                    DTW has been used to analyze video, audio, and graphics. DTW can be applied to any data that can be represented linearly. One of the applications of DTW is in the field of speech recognition. In this case, DTW can deal with the different speech speeds, which results in better recognition.
                </p>
                <h4>Usage</h4>
                <script src="https://gist.github.com/PMeinshausen/c22f77a7f554ad737ad9.js"></script>
            </div>
            <div id="DWT" style="display:none;">
                <h2 class="text-center">DWT</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    This function implements Mallat’s algorithm which is an iterate algorithm in the Discrete Wavelet Transform (DWT) field, and is designed to apply wavelet transform on multiple sequences simultaneously. The input is typically a set of time series sequences. Each sequence consists of values in a specific order. You specify the wavelet name, transform level, and optionally the extension mode. The result is the transformed sequences in Hilbert space with corresponding component identifier and index. The transformation is also called decomposition.
                </p>
                <p>
                    The result can be filtered to reduce the length of the sequence. And the original sequence can be reconstructed according to the result via IDWT. Thus the function can be adopted in compression and denoise.
                </p>
                <h4>Background</h4>
                <p>
                    DWT is a kind of time-frequency analysis tool for which the wavelets are discretely sampled. DWT is different from the Fourier transform, which provides frequency information on the whole time domain. A key advantage of DWT is that it provides frequency information at different time points.
                </p>
                <p>
                    Mallat’s algorithm can be described as several iterative steps. For example, in the case of a 3- level wavelet transform, the algorithm performs these steps:
                </p>
                <p>
                    <span style="font-weight:bold">1)</span> Use <span style="font-style:italic">S(n)</span> as the original time domain sequence as the input of level 1.<br>
                    <span style="font-weight:bold">2)</span> The input sequence is convolved with high-pass filter <span style="font-style:italic">h(n)</span> and low-pass filter <span style="font-style:italic">g(n)</span>, then the convolved sequences are downsampled respectively.
                    Two sequences are generated. The generated sequences are the detail coefficients <span style="font-style:italic">Dk</span> and the approximation coefficients <span style="font-style:italic">Ak</span> in level <span style="font-style:italic">k</span>.<br>
                    <span style="font-weight:bold">3)</span> If current level <span style="font-style:italic">k</span> reaches max transform level <span style="font-style:italic">n</span>, then go to the end. Otherwise, <span style="font-style:italic">Ak</span> is used as the input sequence for the next level; increase the current level <span style="font-style:italic">k</span> by 1, then go to step 2.
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/4a41721519692a9e3ec4.js"></script>
            </div>
            <div id="DWT2D" style="display:none;">
                <h2 class="text-center">DWT2D</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    This function implements wavelet transform on 2 dimensional input, and it is designed to apply wavelet transform on multiple sequences simultaneously. Typically, each sequence is a matrix contains the position in 2 dimensional space and the corresponding values.
                </p>
                <p>
                    Each row in a sequence consists of the sequence identifier (optional), the index of Y axis, the index of X axis, and the corresponding one or more values. Then, you specify the wavelet name, transform level, and optionally the extension mode.
                </p>
                <p>
                    The result is the transformed sequences in Hilbert space with the corresponding component identifier and index. The transformation is also called decomposition as the result is given in multiple components.
                </p>
                <p>
                    A typical use case of DWT2d is:<br>
                    <span style="font-weight:bold">1)</span>Apply DWT2d to the original data and generate the coefficients of matrices and corresponding meta-data.<br>
                    <span style="font-weight:bold">2)</span>Filter the coefficients by various methods (for example, minimum threshold, top n coefficients or just keep the approximate coefficients) according to the object.<br>
                    <span style="font-weight:bold">3)</span>Reconstruct the matrices from the filtered coefficients and compare them with the original ones.<br>
                <h4>Background</h4>
                <p>
                    DWT is a kind of time-frequency analysis tool for which the wavelets are discretely sampled. DWT is different from the Fourier transform, which provides frequency information on the whole time domain. A key advantage of DWT is that it provides frequency information at different time points, which is especially useful to keep local information.
                </p>
                <p>
                    Similar to 1-dimensional DWT, Mallat’s algorithm can be represented as the following iterative steps.                </p>
                <p>
                    <span style="font-weight:bold">1)</span> Use the original time domain sequence (2 dimensional matrix) as the input of level 1.<br>
                    <span style="font-weight:bold">2)</span> In level k, each row of the input matrix is convolved with low-pass filter g(n) and high-pass filter h(n), then each convolved rows are downsampled by column, respectively. Thus, two matrices are generated. After that, each column of the two generated matrices are convolved with the filters g(n) and h(n), and downsampled again. Finally four matrices are given, they are marked as approximation coefficients Ak, horizontal detail coefficients Hk,vertical detail coefficients Vk and diagonal detail coefficients Dk for level n, respectively.<br>
                    <span style="font-weight:bold">3)</span> If current level k reaches max transform level n, then go to the end. Otherwise, Ak is used as the input matrix of next level; increase the current level k by 1, then go to step 2.
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/6eaa5447042379f55951.js"></script>
            </div>
            <div id="FrequentPaths" style="display:none">
                <h2 class="text-center">DWT2D</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    FrequentPaths is a function for mining frequent subsequences as patterns in a sequence database. It has broad applications including the analyses of customer purchase behavior, web access patterns, disease treatments, DNA sequences, and so on.
                </p>
                <h4>Background</h4>
                <p>
                    In a sequential pattern mining application, each sequence consists of an ordered list of itemsets, and each itemset contains at least one item. The items are unordered if there are more than one item in one itemset.
                </p>
                <p>
                    If you use a letter in lower case to represent an item, use "()" to enclose items in an itemset and "&lt;&gt;" to enclose itemsets in a sequence, then the sequence "&lt;(a), (b,c), (d)&gt;" is identical to "&lt;(a),(c,b),(d)&gt;" because items in one itemset are unordered, while sequence "&lt;(a),(b,c),(d)&gt;" is different from "&lt;(a),(d),(b,c)&gt;" because the itemsets are ordered. In web click stream analysis, there is only one item in each itemset, and in purchase behavior analysis there might be multiple items in one itemset as a customer might buy more than one item in one shopping session.                
                </p>
                <p>
                    In sequential pattern mining, the conditions for a sequence α to be a subsequence of another sequence β are:
                    <ul>
                        <li>Each itemset ai in α must be a subset of another itemset bj in β.</li>
                        <li>The ai elements in α must have the same order as the bj elements in β.</li>
                    </ul>
                </p>
                <p>
                    A more formal definition is: a sequence α=a1a2...an is called as a subsequence of another sequence β=&lt;b1b2...bm&gt; and β a super sequence of α, if there exists integers 1≤j1&lt;j2&lt;...≤jn≤m such that a1⊆bj1,a2⊆bj2,...,an⊆bjn.
                </p>
                <p>
                    The support of a sequence α in a sequence dataset SDB is defined as the number of sequences in SDB that contain α (that is, they are super sequences of α). 
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/645dab9807b4f9f54146.js"></script>
            </div>
            <div id="IDWT" style="display:none">
                <h2 class="text-center">IDWT</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    IDWT is the inverse function of DWT. IDWT applies inverse wavelet transforms on multiple sequences simultaneously. The inputs are the coefficients of sequences with specified order outputted by DWT and the corresponding wavelet information used in DWT. After running IDWT, the output sequences are the sequences in time domain. Because the output is comparable with the input of DWT, the transformation is also called reconstruction.
                </p>
                <h4>Background</h4>
                <p>
                    These steps describe a typical use case of IDWT:
                </p>
                <ol>
                    <li>Apply DWT to sequences, generate the coefficients of sequences and corresponding meta data.</li>
                    <li>Filter the coefficients by various methods (for example, minimum threshold and top n coefficients) according to the object.</li>
                    <li>Reconstruct the sequences from the filtered coefficients and compare them with the original ones.</li>
                </ol>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/b0e0bdbab4399c948543.js"></script>
            </div>
            <div id="IDWT2D" style="display:none">
                <h2 class="text-center">IDWT2D</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    IDWT2d is the inverse function of DWT2d. Or, more specifically, it applies inverse wavelet transforms on multiple sequences simultaneously. The inputs are the coefficients of sequences with specified order outputted by DWT and the corresponding wavelet information generated during the transformation. After running IDWT2d, the output sequences are the original sequences in matrix forms. As the output is comparable with the input of DWT, the transformation is also called reconstruction.
                </p>
                <h4>Background</h4>
                <p>
                    Similar to IDWT, a typical use case of IDWT2d consists of the following steps:
                </p>
                <ol>
                    <li>Apply DWT2d to the original data, generate the coefficients of matrices and corresponding meta data.</li>
                    <li>Filter the coefficients by various methods (for example, minimum threshold and top n coefficients) according to the object. Thus we may compress the original matrices.</li>
                    <li>Reconstruct the matrices from the filtered coefficients and compare them with the original ones.</li>
                </ol>
                <p>
                    Also, there exists other cases. For example, you may find the flaws in the transformed domain according to the energy of different components and map it back to the original position according to the indexes.
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/39a4960a9c5c3b60d1f5.js"></script>
            </div>
            <div id="Path_Analyzer" style="display:none">
                <h2 class="text-center">Path_Analyzer</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The Path_Analyzer function automates path analysis. This function acts as a wrapper function of the following three path functions, which you can use to perform clickstream analysis of common sequences of user pageviews on websites:
                </p>
                <ul>
                    <li>Path_Generator—Generates all the possible paths (sequences of pageviews on a website).</li>
                    <li>Path_Start—Generates all the child paths for a particular parent path and sums up the count of times each child path was traveled by a user.</li>
                    <li>Path_Summarizer—Counts the number of times various paths were traveled and measures the depth in pageviews of each path.</li>
                </ul>
                <p>
                    The Path_Analyzer function is useful for website clickstream analysis of website traffic and other sequence/path analysis tasks such as advertisement or referral attribution.
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/bfae79eddf8216b2816d.js"></script>
            </div>
            <div id="Path_Generator" style="display:none">
                <h2 class="text-center">Path_Generator</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    This function takes as input a set of paths where each path is a route (series of pageviews) taken by a user in a single session from the start of the session until its end. For each path, Path Generator generates the correctly formatted sequence and all possible sub-sequences for further analysis by the Path Summarizer function. The first element in the path is the first page a user could visit. The last element of a path is the last page visited by the user.
                </p>
                <p>
                    Together, the Path Generator, Path Summarizer, and Path Starter functions are used to perform clickstream analysis of common sequences of users’ pageviews on a website. The roles of the three functions are:
                </p>
                <ul>
                    <li>Path Generator generates all the possible paths (sequences of pageviews on a website);</li>
                    <li>Path Summarizer counts the number of times various paths were traveled and measures the depth in pageviews of each path; and</li>
                    <li>Path Starter generates all the child paths for a particular parent path and sums up the count of times each child path was traveled.</li>
                </ul>
                <p>
                    In the discussion below, we will use the terms:
                </p>
                <ul>
                    <li>Path: An ordered, start-to-finish series of actions (for example, pageviews) for which you wish to generate sequences and sub-sequences. You will run Path Generator on the set of all observed paths users have traveled while navigating your website.</li>
                    <li>Sequence: The sequence is the path prefixed with a carat ('^') to indicate the start of the path. For example, if a user visited page a, page b, and page c in that order, we would say that his session had the sequence, ^,a,b,c.</li>
                    <li>Sub-sequence: For a given sequence of actions, a sub-sequence is one possible subset of the steps that begins with the initial action. For example, the path a,b,c generates three sub- sequences: ^,a; ^,a,b; and ^,a,b,c.</li>
                </ul>
                <h4>Background</h4>
                <p>
                    This tool is useful for performing clickstream analysis of website traffic. These functions can also be used for doing other types of sequence/path analysis, such as the analysis required for advertisement attribution and referral attribution.
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/6c2a3f97f9993d15747d.js"></script>
            </div>
            <div id="Path_Start" style="display:none">
                <h2 class="text-center">Path_Start</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The output of Path Summarizer function is the input to this function. This function generates all the children for a particular parent and sums up their count. Note that the input data has to be partitioned by the parent column.
                </p>
                <h4>Background</h4>
                <p>
                    This function is useful for website clickstream analysis and other sequence/path analysis tasks such as advertisement attribution.
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/697ef4725b9a6ef682fc.js"></script>
            </div>
            <div id="Path_Summarizer" style="display:none">
                <h2 class="text-center">Path_Summarizer</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The output of the Path Generator function is the input to this function. This function is used to sum counts on nodes. A node can either be a plain sub-sequence or an exit sub-sequence. An exit sub-sequence is one in which the sequence and the sub-sequence are the same. Exit sub- sequences are denoted by appending a dollar sign ('$') to the end of the sequence.
                </p>
                <h4>Background</h4>
                <p>
                    This function is useful for website clickstream analysis and other sequence/path analysis tasks such as advertisement attribution.
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/b94bebb64731887a6473.js"></script>
            </div>
            <div id="SAX" style="display:none">
                <h2 class="text-center">SAX</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    Symbolic Aggregate approXimation (SAX) transforms original time series data into symbolic strings. Once this transformation is complete, the data is more suitable for many additional types of manipulation, both because of its smaller size and the relative ease with which patterns can be identified and compared.                
                </p>
                <h4>Background</h4>
                <p>
                    A time series is a collection of data observations made sequentially in time. Time series occur in virtually every medical, scientific, entertainment and business domain.
                </p>
                <p>
                    Symbolic Aggregate approXimation uses a simple algorithm, with low computational complexity to create symbolic strings from time series data. Using a symbolic representation enables additional functions (for example, Teradata Aster nPath) to easily operate on the data.
                </p>
                <p>
                    The data can also be manipulated using common algorithms such as hashing or regular expression pattern matching. In classic data mining tasks such as classification, clustering, and indexing, SAX is accepted as being as good as some well-known, but storage-intensive methods like Discrete Wavelet Transform (DWT) and Discrete Fourier Transform (DFT).
                </p>
                <p>
                    SAX transforms a time series X of length n into the string of arbitrary length w, where w &lt; n, using an alphabet A of size a &gt; 2.
                </p>
                <p>
                    The SAX algorithm consists of two steps:
                </p>
                <ol>
                    <li>SAX transforms the original time series data into a PAA (Piecewise Aggregate Approximation) representation. This effectively splits the time series data into intervals, and then assigns each interval to one of a limited set of alphabetical symbols (letters) based on the data being examined. The set of symbols used is based on dividing all observed data into “chunks” or thresholds, using the normal distribution curve. Each of these values, or “chunks” is represented by a symbol (a letter). This is a simple way to reduce the dimensionality of the data.</li>
                    <li>The PAA is then converted into a string consisting of letters that represents the patterns occurring in the data over time.</li>
                </ol>
                <p>
                    The symbols created by SAX correspond to the time series features with equal probability, allowing them to be compared and used for further manipulation with reliable accuracy. The time series that are normalized using the zero mean and unit of energy follow the Normal distribution law. By using Gaussian distribution properties, it's easy for SAX to pick equal- sized areas under the Normal curve using lookup tables for the cut lines coordinates, slicing the under-the-Gaussian-curve area. The x coordinates of these lines are called “breakpoints” in the SAX algorithm context.
                </p>
                <h4>Usage</h4>
                </div>
                <ul class="nav nav-tabs" role="tablist">
                    <li class="active"><a href="#partition_function" role="tab" data-toggle="tab">Partition function</a></li>
                    <li><a href="#multiple_input_function" role="tab" data-toggle="tab">Multiple Input</a></li>
                </ul>
                <div class="tab-content">
                    <div class="tab-pane active" id="partition_function">
                        <script src="https://gist.github.com/PMeinshausen/36c1efebda6641de5c16.js"></script>
                    </div>
                    <div class="tab-pane" id="multiple_input_function">
                        <script src="https://gist.github.com/PMeinshausen/b1cd8157575e4337bdf7.js"></script>
                    </div>
                </div>
            </div>
            <div id="Sessionization" style="display:none">
                <h2 class="text-center">Sessionization</h2>
                <div class="text_scroll">
                <h4>Background</h4>
                <p>
                    Sessionization is the process of mapping each click in a clickstream to a unique session identifier. We define a session as a sequence of clicks by a particular user where no more than n seconds pass between successive clicks (that is, if we don’t see a click from a user for n seconds, we start a new session).
                </p>
                <p>
                    Sessionization can be easily done with the Sessionize SQL-MapReduce function. Sample code is included with the Aster SQL-MapReduce Java API. This sessionize SQL-MapReduce function can also be used to detect web crawler (“bot”) activity. If the time between successive clicks is less than the user-specified threshold, bot activity will be flagged.
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/4cacd81e23f66a08c208.js"></script>
            </div>
            <div id="nPath" style="display:none">
                <h2 class="text-center">What is Teradata Aster nPath?</h2>
                <div class="text_scroll">
                <p>
                    The Teradata Aster nPath SQL-MR function allows you to perform regular pattern matching over a sequence of rows from one or more inputs. For clarity, this document refers to each sequence of matched rows as a matched pattern.                
                </p>
                <p>
                    With Teradata Aster nPath, you can find sequences of rows that match a pattern of your choice. The Teradata Aster nPath function lets you use symbols when building patterns. Symbols let you define the pattern matching conditions and help you extract information from these matched rows.
                </p>
                <p>
                    The Teradata Aster nPath function lets you:
                </p>
                <ol>
                    <li>use a regular expression to specify a pattern you want to match in an ordered collection of rows and label individual matching rows with symbols; and</li>
                    <li>compute SQL aggregates on or find particular values in each matched pattern (Teradata Aster nPath uses the symbols you define for matching rows to get these aggregates and values).</li>
                <ol>
                <p>
                    The Teradata Aster nPath function uses regular expressions because they are simple, widely understood, and flexible enough to express most search criteria. While most uses of regular expressions focus on matching patterns in strings of text; Teradata Aster nPath enables matching patterns in sequences of rows.
                </p>
                <p>
                    The Teradata Aster nPath function performs fast analysis on ordered data. The clauses in Teradata Aster nPath let you express complicated pathing queries and ordering relationships that might otherwise require you to write multi-level joins of relations in SQL. With Teradata Aster nPath, you indicate a desired ordering and then specify a pattern that is matched across the ordered rows of data.
                </p>
                <p>
                    For each matched pattern in the sequence of rows, Teradata Aster nPath generates a row of output that contains SQL aggregates computed over the rows in the matched pattern.
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/b35dea788fbcaa25c756.js"></script>
            </div>
            <div id="Approximate Distinct Count" style="display:none">
                <h2 class="text-center">Approximate Distinct Count</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    Based on probabilistic counting algorithms, this function quickly estimates the number of distinct values in a column or combination of columns, while scanning the table only once.
                </p>
                <p>
                    For a column or column combination with large cardinality, it can calculate an approximate count of the distinct values in much less time than would be required to calculate a precise distinct count using SQL’s DISTINCT.
                </p>
                <h4>Background</h4>
                <p>
                    For more information about Probabilistic Counting Algorithms, see Probabilistic Counting Algorithms for Data Base Applications, by Philippe Flajolet and G. Nigel Martin (http://portal.acm.org/citation.cfm?id=5215).
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/f88634b0833ff41051e7.js"></script>
            </div>
            <div id="Approximate Percentile" style="display:none">
                <h2 class="text-center">Approximate Percentile</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    This function computes approximate percentiles for one or more columns of data. The accuracy of the approximation is a parameter the user can vary. Higher accuracy requires longer compute time and vice versa. Optionally, you can specify a column to group by, to compute approximate percentiles over different groups.
                </p>
                <h4>Background</h4>
                <p>
                    The function is based on an algorithm developed by Greenwald and Khanna. It gives e- approximate quantile summaries of a set of N elements, where e is the value you specify as the function’s ERROR parameter. Given any rank r, an e-approximate summary returns a value whose rank r' is guaranteed to be within the interval [r - eN, r + eN]. The algorithm has a worst case space requirement of O((1/e) * log(eN)).
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/24e5a689047d60973370.js"></script>
            </div>
            <div id="ConfusionMatrix" style="display:none">
                <h2 class="text-center">ConfusionMatrix</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    In the field of artificial intelligence, a confusion matrix is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix).
                </p>
                <p>
                    Each column of the matrix represents the instances in a predicted class, while each row represents the instances in an actual class.
                </p>
                <p>
                    The name stems from the fact that it makes it easy to see if the system is confusing two classes (that is, commonly mislabeling one as another). Outside artificial intelligence, the confusion matrix is often called the contingency table or the error matrix.
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/9bba8d071aa79ee4aec3.js"></script>
            </div>
            <div id="Correlation" style="display:none">
                <h2 class="text-center">Correlation</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The correlation functions, Corr_Reduce and Corr_Map, compute a global correlation between any pair of columns (COLUMNPAIRS) from a table. You may run this pair of functions on multiple pairs of columns in a single invocation. Measuring correlation allows you to determine if the value of one variable is useful in predicting the value of another.
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/f70803be05ca96251f4d.js"></script>
            </div>
            <div id="Distribution Matching" style="display:none">
                <h2 class="text-center">Distribution Matching</h2>
                <div class="text_scroll">
                <p>
                    The distribution matching function distnmatch carries out hypothesis testing and finds the best matching distribution for the data
                </p>
                <h4>Summary</h4>
                <p>
                    The distnmatch function tests the hypothesis that the sample data comes from the specified reference distribution. In this mode, this function carries out up to four tests simultaneously:
                </p>
                <ul>
                    <li>Anderson-Darling test</li>
                    <li>Kolmogorov-Smirnov test</li>
                    <li>Cramer-von Mises criterion</li>
                    <li>Pearson's Chi-squared test</li>
                </ul>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="EMAVG" style="display:none">
                <h2 class="text-center">EMAVG</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The exponential moving average function, EMAVG, computes the average over a number of points in a time series but applies a damping (weighting) factor to older values. The weighting for the older values decreases exponentially without entirely discarding the older values.
                </p>
                <h4>Background</h4>
                <p>
                    Exponential moving average (EMA), sometimes also called an exponentially weighted moving average (EWMA), applies weighting factors that decrease exponentially. The weighting for each older data point decreases exponentially, giving much more importance to recent observations while still not discarding older observations entirely.
                </p>
                <p>
                    We compute the arithmetic average of the first n rows as specified by START_ROWS argument. Then, for each subsequent row, we compute the new value as:
                </p>
                <p style="font-family:Latin Modern Mono">
                    new_emavg = alpha * new_value + (1-alpha) * old_emavg
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/feffd707471e071bd1df.js"></script>
            </div>
            <div id="Enhanced Histogram Function" style="display:none">
                <h2 class="text-center">Enhanced Histogram Function</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The enhanced histogram function (represented by Hist_Map and Hist_Reduce) adds these new capabilities over the existing histogram function (represented by histogram_map and histogram_reduce):
                </p>
                <ul>
                    <li>Bin selection—The new function is equipped with algorithms that determine the number of equi-spaced bins without your intervention.</li>
                    <li>Non-integer bin ranges—The new function supports non-integer bin ranges.</li>
                    <li>Bin IDs and bin ranges in the output columns—By default, the output columns include bin IDs, bin ranges, and bin percentages.</li>
                    <li>Bin ranges—The new function supports left-inclusive and right-inclusive bin ranges in the new function.</li>
                    <li>Bin breaks—The bin breaks generated by the function are always aligned to multiples of a power of ten or multiples of five times a power of ten.</li>
                    <li>Multiple histograms—You can build separate histograms for distinct groups by values.</li>
                </ul>
                </div>
            </div>
            <div id="FMeasure" style="display:none">
                <h2 class="text-center">FMeasure</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The FMeasure function calculates the accuracy of a test (usually the output of a classifier).
                </p>
                <h4>Background</h4>
                <p>
                    In statistics, the F1 score (also F-score or F-measure) is a measure of a test’s accuracy. It considers both the precision p and the recall r of the test to compute the score:
                </p>
                <ul>
                    <li>p is the number of correct results divided by the number of all returned results.</li>
                    <li>r is the number of correct results divided by the number of results that should have been returned.</li>
                </ul>
                <p>
                    The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and its worst score at 0.
                </p>
                <p>
                    The traditional F-measure or balanced F-score (F1 score) is the harmonic mean of precision and recall:
                </p>
                <p>
                    F =2*precision*recall / (precision+recall)</br>
                    The general formula for a positive real β is:</br>
                    Fβ =(1+β*β)*precision*recall /(β*β*precision+recall)
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/84d802a638bc4261c221.js"></script>
            </div>
            <div id="GLM" style="display:none">
                <h2 class="text-center">GLM</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    Generalized linear model (GLM) is an extension of the linear regression model that enables the linear equation to be related to the dependent variable(s) via a link function. GLM performs linear regression analysis for any of a number of distribution functions using a user- specified distribution family and link function. The link function is chosen based upon the distribution family used and the assumed nonlinear distribution of expected outcomes. Supported link models in Aster Database are ordinary linear regression, logistic regression (logit model), and Poisson log-linear model.
                </p>
                <p>
                    A GLM has three parts:
                </p>
                <ol>
                    <li>A random component - the probability distribution of Y from the exponential family</li>
                    <li>A fixed linear component - the linear expression of the predictor values (X1,X2,...,Xp),expressed as η or Xβ</li>
                    <li>A link function that describes the relationship of the distribution function to the expected value of Y (for example, linear regression, logistic or logit regression, or Poisson loglinear model)</li>
                </ol>
                <p>
                    GLM adds support to categorical variables. In the following table “outcome” is the dependent variable, and “weight,” ‘color,” and “size” are the independent variables, where “weight” is a quantitative variable and “color” is a qualitative one.
                </p>
                <p>
                    In regression analysis, the qualitative variable is called a categorical (or dummy) variable. If a variable is categorical and has n category values, it is extended into n-1 dummy variables, where each dummy variable is a binary indicator for the corresponding category value.
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/1b3050da484c459e4151.js"></script>
            </div>
            <div id="GLMPredict" style="display:none">
                <h2 class="text-center">GLMPredict</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    This function performs generalized linear model prediction. It lets you use the model generated by the GLM function to predict new input data.
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/34b95b2b60ce500e8b74.js"></script>
            </div>
            <div id="Histogram" style="display:none">
                <h2 class="text-center">Histogram</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The histogram function maps each input row to one or more bins based on criteria you specify and returns the row count for each bin.
                </p>
                <p>
                    The SQL-MapReduce histogram function is a combination of SQL-MapReduce row function (histogram_map) and an SQL-MapReduce partition function (histogram_reduce). The output of the histogram function is useful for assessing the shape of a data distribution.
                </p>
                <h4>Usage</h4>
                <p>
                    In order to generate a histogram on an input data set you must run a map and reduce step on the data. Below is the syntax for running the map phase.
                </p>
                </div>
                <script src="https://gist.github.com/PMeinshausen/f9874a20669891a84ad2.js"></script>
            </div>
            <div id="KNN" style="display:none">
                <h2 class="text-center">KNN</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The KNN function is optimized for small training sets that fit in memory and for large sets. This function supports:                
                </p>
                <ul>
                    <li>User-defined distance metrics.<br>
                        By default, the KNN function uses the Euclidean distance metric to determine the proximity to training data objects. However, the KNN function lets you define your own distance metric.</li>
                    <li>Distance-weighted voting.<br>
                        The KNN function uses distance-weighted voting to help improve the classification accuracy. The KNN function calculates distance-weighted voting (w) using this equation:<br>
                        w = 1/pow(distance, voting_weight)<br>
                        Where distance is the distance between the test object and the training object, and voting_weight is an argument that you pass to the KNN function.</li>
                </ul>
                <h4>Background</h4>
                <p>
                    In the IEEE International Conference on Data Mining (ICDM) in December 2006, the K- Nearest Neighbor (kNN) classification algorithm was presented as one of the top 10 ten data mining algorithm.
                </p>
                <p>
                    The kNN algorithm is a technique for classifying data objects based on proximity to other data objects with known classification. The object with known classification or labels serve as training data.
                </p>
                <p>
                    kNN classifies data based on the following parameters:
                </p>
                <ul>
                    <li>Training data.</li>
                    <li>A metric that measures distance between objects.</li>
                    <li>The number of nearest neighbors (k).
                </ul>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/1e940e85ebe76278d186.js"></script>
            </div>
            <div id="LARS Functions" style="display:none">
                <h2 class="text-center">LARS Functions</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    Least Angle Regression (LARS) and its most important modification, least absolute shrinkage and selection operator (LASSO), are attractive variants of linear regression that select the most important variables, one by one, and fit the coefficients dynamically. Aster Database provides two LARS functions: lars and LARSPredict.               
                </p>
                <p>
                    The output of the lars function is used by the LARSPredict function to make predictions.
                </p>
                <h4>Background</h4>
                <p>
                    LARS is a model-selection algorithm, is a useful and less greedy version of traditional forward selection methods. LASSO is an attractive version of ordinary least squares (OLS) that constrains the sum of the absolute regression coefficients. LASSO is an important sparse learning method.
                </p>
                <p>
                    LASSO is a linear-regression-like method, estimating the coefficients for each input variable, which is used to make predictions for the response variable. However, compared to ordinary least squares, LASSO does the fitting in a smarter way. It always finds the most significant variables (have the greatest absolute correlation with the current residuals) in a sequential manner. In other words, it performs the variable selection job.
                </p>
                <p>
                    This form of fitting is very useful when the you have thousands of input variables. The time complexity is the same as running linear regression, which is linear in the number of rows. In addition, LASSO can still work in some situations in which ordinary least squares cannot, such as when there is multicollinearity.
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/b450a5cae39b672ba909.js"></script>
            </div>
            <div id="LinReg" style="display:none">
                <h2 class="text-center">LinReg</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    Outputs coefficients of the linear regression model represented by the input matrices. The zeroth coefficient corresponds to the slope intercept.                
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/7925a551c8bf50f563dd.js"></script>
            </div>
            <div id="Percentile" style="display:none">
                <h2 class="text-center">Percentile</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    This analytical function can be used to find percentiles on a per group basis.               
                </p>
                <h4>Background</h4>
                <p>
                    This function generates the exact percentiles for a group of numbers. This function works well when the number of groups is big (in the order of > 100s of thousands) and each group is small enough to fit in memory (in the order of 10s of thousands). The exact number depends on the cluster configuration.
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/e8891d2c8c5668c290e5.js"></script>
            </div>
            <div id="Principal Component Analysis" style="display:none">
                <h2 class="text-center">Principal Component Analysis</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    Principal component analysis (PCA) is a common unsupervised learning technique that is useful for both exploratory data analysis and dimensionality reduction. It is often used as the core procedure for factor analysis.              
                </p>
                <h4>Background</h4>
                <p>
                    The PCA is a dimension reduction technique. In a lot of cases you may have thousands of input variables. There is a high probability that some of these variables are linearly correlated. Some statistical analysis tools, such as linear regression, do not allow these kind of linearly correlated inputs. Also, high dimensionality causes a lot of problems in the application of statistical tools (see ‘curse of dimensionality’). For all these reasons, it is often desirable to reduce the thousands of potentially linearly correlated input variables to a few linearly uncorrelated variables, which are called principal components. This is what PCA does.
                </p>
                <p>
                    PCA takes an N × M data matrix (N observations, M variables), and generates an M × M “rotation matrix.” Each column of the rotation matrix represents an axis in M-dimensional space. The first k columns are the k dimensions along which the data varies most, and thus in some cases can be considered the most important. We can throw away the remaining M – k columns, and we are left with a M × k rotation matrix. To get the values of our dataset in the coordinate system of our principal components, we multiply the original N × M dataset by the M × k rotation matrix to get a final N × k matrix. This matrix represents our dataset with a reduced dimensionality of k ≤ M
                </p>
                <p>
                    As for the results, each eigenvector (output row, less the last standard deviation column) is a weighting scheme over the original input variables, which means the linear combination of the original variables using this eigenvector is a principal component. (Notice that the length of the eigenvector is the same as the number of the original input variables so the multiplication does work.) By choosing the first k eigenvectors, we get k principal components with the k highest standard deviations (due to the eigenvector computation). These principal components are linearly uncorrelated and can be used in further analysis (as input variables).
                </p>
                <p>
                    Note that the rank of principal components decreases in standard deviation, thus significance. Usually the first several principal components would explain 80%–90% of the total variance, which is sufficient in most applications. That is how ‘dimension reduction’ works.
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/1c4f9b8cba052c42bee7.js"></script>
            </div>
            <div id="SMAVG" style="display:none">
                <h2 class="text-center">SMAVG</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The SMAVG function computes the average over a number of points in a series.               
                </p>
                <h4>Background</h4>
                <p>
                    A Simple Moving Average (SMA) is the unweighted mean of the previous n data points. For example, a 10-day simple moving average of closing price is the mean of the previous 10 days' closing prices.
                </p>
                <p>
                    To calculate this, we compute the arithmetic average of first R rows as specified by the WINDOW_SIZE argument. Then, for each subsequent row, compute new value as
                </p>
                <p style="font-family:Latin Modern Mono">
                    new_smavg = old_smavg - (PM-n+PM) / N
                </p>
                <p>
                    where N is the number of rows as specified by the WINDOW_SIZE argument.
                </p>
                <h4>Usage</h4>
                </div>
                <script src="https://gist.github.com/PMeinshausen/f379363bb665d175ecaf.js"></script>
            </div>
            <div id="Sample" style="display:none">
                <h2 class="text-center">Sample</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The sample function draws rows randomly from the input table. The function offers two sampling schemes:              
                </p>
                <ul>
                    <li>A simple Bernoulli (Binomial) sampling on a row-by-row basis with given sample rates</li>
                    <li>Sampling without replacement that selects a given number of rows</li>
                </ul>
                <p>
                    The sampling can either be applied to the entire relation, which is called unconditional sampling, or be refined with conditions, which is called conditional sampling.
                </p>
                <p>
                    Note that only one random number generator is used throughout a single task for unconditional sampling, whereas a separate random number generator is created for each condition in one task for conditional sampling.
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="Support Vector Machines" style="display:none">
                <h2 class="text-center">Support Vector Machines</h2>
                <div class="text_scroll">
                <p>
                    The Support Vector Machines (SVM) suite of functions consists of these functions:             
                </p>
                <ul>
                    <li>SparseSVMTrainer—Builds a predictive model according to a training set.</li>
                    <li>SparseSVMPredictor—Gives a prediction for each sample in the test set.</li>
                    <li>SVMModelPrinter—Displays the readable information of the model.</li>
                </ul>
                <h4>Background</h4>
                <p>
                    Support vector machines are among the most popular "out of the box" classification algorithms. The objective of the algorithm is similar to logistic regression – given a set of predictor variables, classify an object as one of two possible outcomes. The methods by which the algorithms achieve this objective are very different. Intuitively, logistic regression develops a probabilistic model based on the input data, and given a test instance x, estimates the probability that x belongs in a particular class. In contrast, support vector machines ignore the probabilistic interpretation. A support vector machine attempts to find the boundary that maximizes the distance between the two classes. In the prediction phrase, given a test instance x, calculate which side of the boundary x lies in order to compute a class prediction.
                </p>
                <p>
                    This implementation solves the primal form of a linear kernel support vector machine via gradient descent on the objective function. It is based primarily on Pegasos: Primal Estimated Sub-GrAdient SOlver for SVM (by S. Shalev-Shwartz, Y. Singer, and N. Srebro; presented at ICML 2007).
                </p>
                <p>
                    As mentioned before, it is a binary classification algorithm. Multiple classification is achieved using machine learning reductions, or more specifically one-against-all is adopted in the function. In a K-class classification problem, K support vector machines are trained. Each SVM is a binary classifier, where the nth class is labeled positive, and all other classes are labeled negative. In the test phase, each test observation is trained using each of the K SVMs. The class which results in the most observations predicted as positive is the resulting prediction.
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="VWAP" style="display:none">
                <h2 class="text-center">Volume-Weighted Average Price (VWAP)</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    This function computes, for each in a series of equal-length intervals, the volume-weighted average price of a traded item (usually an equity share). You specify the interval length in the TIMEINTERVAL argument.
                </p>
                <p>
                    The first interval starts at the time of the earliest timestamp in the partition, and it ends with the last row timestamped less than TIMEINTERVAL seconds later. The second interval starts immediately after the end of the first, and so on. All intervals have the same length.
                </p>
                <h4>Background</h4>
                <p>
                    Volume-Weighted Average Price (VWAP) computes the sum of the product of the volume and price divided by the total volume traded in a specified window:
                </p>
                <p style="font-family:Latin Modern Mono">
                    VWAP  =  sum(vol*price)/sum(vol)
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="WMAVG" style="display:none">
                <h2 class="text-center">WMAVG</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The weighted moving average computes the average over a number of points in a time series but applies a weighting to older values. The weighting for the older values decreases arithmetically.
                </p>
                <h4>Background</h4>
                <p>
                    A weighted average is any average that has multiplying factors to give different weights to different data points. Mathematically, the moving average is the convolution of the data points with a moving average function. In technical analysis, a weighted moving average (WMA) has the specific meaning of weights that decrease arithmetically. In an n-day WMA, the latest day has weight n, the second latest has (n - 1), and so on, counting down to zero.
                </p>
                <p style="font-family:Latin Modern Mono">
                    Total_[M+1] = Total_[M] + P_[M+1] - P_[M-n+1]<br>
                    Numerator_[M+1] = Numerator_[M] +n*P_[M+1] - Total[M]<br>
                    new_WMAVG  = Numerator_[M+1]/(n(n+1)/2)
                </p>
                <p>
                    Where n is the number of rows as specified by the WINDOW_SIZE argument.
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="LDA Functions" style="display:none">
                <h2 class="text-center">LDA Functions</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The LDA suite of functions consists of these functions:            
                </p>
                <ul>
                    <li>LDATrainer</li>
                    <li>LDAInference</li>
                    <li>LDATopicPrinter</li>
                </ul>
                <p>
                    In a typical case, you use the LDATrainer function to build a topic model based on the supplied training data and parameters. Then, you use LDAInference to estimate the topic distribution for each document based on the model generated by LDATrainer. To display the readable information of the model, use the LDATopicPrinter function.
                </p>
                <h4>Background</h4>
                <p>
                    Topic model is a state-of-art technology in text analysis. Topic model assumes that a document consists of multiple abstract topics with corresponding probabilities. Each topic emits a list of words with specific probability. That is to say, a word in a given document is generated by a topic with certain probability decided by the topic, and the probability of the topic is decided according to the document.
                </p>
                <p>
                    In the model, a document may contain many topics. For example, it might contain the words “rainy” and “sunny,” which are related to weather. It might also contain the words “basketball” and “football,” which related to sports. If 20% of a document is about weather and the remaining part is about sports, there would probably be about 4 times more sports-related words than the words describing weather. Topic model is used to obtain the latent factors based on a statistical framework.
                </p>
                <p>
                    Latent Dirichlet Allocation (LDA) is a well-known generative model. It was first introduced in the article Latent Dirichlet Allocation. In an LDA model, the terms topic probabilities and topic-document probabilities are modeled with a Dirichlet distribution as a prior.
                </p>
                <p>
                    As a Bayesian method, the main advantage of LDA is that it is not as susceptible to overfitting and works well for smaller datasets. LDA has been successfully used in text modeling, content- based image retrieval, and bioinformatics.
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="Levenshtein Distance" style="display:none">
                <h2 class="text-center">Levenshtein Distance</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    This function computes the Levenshtein distance between two text values. The Levenshtein distance (sometimes called the “edit distance”), computes the number of edits needed to transform one string into the other, where edits include insertions, deletions, or substitutions of individual characters.This computation is useful for fuzzy matching of sequences and strings. It is one measure used to compare how “far apart” two strings are.
                </p>
                <h4>Background</h4>
                <p>
                    This function is frequently used to resolve a user-entered value to a standard value, such as when a person types “Hanning Mankel” when he’s actually searching for Henning Mankell.
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="Naive Bayes Text Classifier" style="display:none">
                <h2 class="text-center">Naive Bayes Text Classifer</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The SQL-MR Naive Bayes Text Classifier is a variant of the Naive Bayes classification algorithm specifically designed for document classification. This classifier executes two functions:             
                </p>
                <ul>
                    <li>NaiveBayesText—Trains a Naive Bayes classification model on a set of documents.</li>
                    <li>NaiveBayesTextPredict—Uses the model generated by NaiveBayesText to predict the category or classification of new documents.</li>
                </ul>
                <h4>Background</h4>
                <p>
                    The Naive Bayes algorithm is a simple, yet very effective technique for classifying data. 
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="Named Entity Recognition" style="display:none">
                <h2 class="text-center">Named Entity Recognition</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    Named entity recognition (NER) is a process of finding instances of specified entities in text. For example, a simple news named-entity recognizer for the English language might find the person mentioned (John J. Smith) and the location mentioned (Seattle) in the text “John J. Smith lives in Seattle”.             
                </p>
                <h4>Background</h4>
                <p>
                    SQL is not a convenient way to do this type of searching. For each type of item you want to find, you would needs to issue an SQL query, or merge together multiple SQL subqueries that are joined by OR operators. In addition, you would need a mechanism to label extracted fields that were found. 
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="nGram" style="display:none">
                <h2 class="text-center">nGram</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The nGram function tokenizes (or splits) an input stream of text and emits n multi-grams (called “n-grams”) based on the specified delimiter and reset parameters. nGram provides more flexibility than standard tokenization when performing text analysis. Many two-word phrases carry important meaning (for example, “machine learning”) that unigrams (single- word tokens) do not capture. This, combined with additional analytical techniques, can be useful for performing sentiment analysis, topic identification, and document classification.             
                </p>
                <p>
                    nGram considers each input row to be one document, and it returns a row for each unique n- gram in each document. Optionally, you can have nGram also return the counts of each n- gram and the total number of n-grams, per document.
                </p>
                <h4>Background</h4>
                <p>
                    General background on tokenization can be found here: http://en.wikipedia.org/wiki/ Lexical_analysis#Tokenizer
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="PoSTagger" style="display:none">
                <h2 class="text-center">PoSTagger</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    This function tags the parts-of-speech of input text.             
                </p>
                <h4>Background</h4>
                <p>
                    Part-of-speech tagging is an important preprocessing step in many natural language- processing applications and the first step in the syntactic analysis of a language.
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="Sentenizer" style="display:none">
                <h2 class="text-center">Sentenizer</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The Sentenizer function extracts the sentences in the input paragraphs.             
                </p>
                <h4>Background</h4>
                <p>
                    Many Natural Language Processing (NLP) processing tasks such as Part-Of-Speech tagging and chunking, begin by identifying sentences. In English, a sentence is ended with a short list of sentence-ending punctuation marks, such as the period (“.”), question mark (“?”), and the exclamation mark (“!”).
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="Sentiment Extraction Functions" style="display:none">
                <h2 class="text-center">Sentiment Extraction Function</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    Sentiment extraction is the process of deducing a user's opinion (positive, negative, neutral) from text-based content. Sentiment extraction is useful for analyzing users’ opinions as found in the content of call centers, forums, social media, and so on.             
                </p>
                <p>
                    The basis for sentiment extraction in the ExtractSentiment function is either a dictionary model, or a model generated using the TrainSentimentExtractor function:
                </p>
                <ul>
                    <li>Dictionary model - developed using a dictionary from WordNet. Note that the word “not”,andseveralothernegationwords,havebeenaddedintothedictionaryas999.These are listed here:</li>
                    <li>Classification model - uses the model you have trained using the TrainSentimentExtractor function.</li>
                </ul>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="TF_IDF" style="display:none">
                <h2 class="text-center">TD_IDF</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    TF-IDF evaluates the importance of a word within a specific document, weighted by the number of times the word appears in the entire corpus of documents.             
                </p>
                <p>
                    Term frequency indicates how often a term appears in a specific document. Inverse document frequency measures the general importance of a term within an entire corpus of documents. That is, each term in the dictionary has an idf score. Each term in each document is given a TF_IDF score, which is equal to tf * idf. A high TF_IDF score for a term generally means that the term is uniquely relevant to a specific document.
                </p>
                <p>
                    To compute TF-IDF values, the TF_IDF SQL-MR function relies on the TF SQL-MR function, which computes the TF value of the input.
                </p>
                <h4>Background</h4>
                <p>
                    TF-IDF stands for “term frequency” - “inverse document frequency.” It is a technique for weighting words in a document. The resulting weights can be used together in a vector space model as input for various document clustering or classification algorithms. Examples include cosine-similarity, latent Dirichlet allocation, k-means clustering, and k-nearest neighbors.
                </p>
                <p>
                    When using a TF-IDF representation of a document, the function models the document as a “bag of words.” The function assumes that the ordering of the words (or n-grams) is not important, and only considers the number of times the word appears. Each document is represented by an N-dimensional vector, where N is the number of words in the dictionary (hence the document vectors are generally very sparse). Each entry in this vector is the TF-IDF score of a word.
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="Text_Classifier" style="display:none">
                <h2 class="text-center">TD_IDF</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    Text classification is the task of choosing the correct class label for a given text input. In basic text classification tasks, each input is considered in isolation from all other inputs, and the set of class labels is defined in advance.
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="Text_Parser" style="display:none">
                <h2 class="text-center">Text_Parser</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    Text parser (“text_parser” formerly “tokenize_cnt”) is a general tool for working with text fields. It can tokenize an input stream of words, optionally stem them, then emit these words in one row, or emit each individual word in a row, with an optional count of the number of times it appears.             
                </p>
                <p>
                    Text parser is widely used to process text data.
                </p>
                <p>
                    For English language text, there are several key parts to take into account when parsing text:
                </p>
                <ul>
                    <li>Punctuating sentences</li>
                    <li>Delimiting a sentence into words</li>
                    <li>Filtering stop words</li>
                    <li>Stemming words</li>
                </ul>
                <p>
                    When invoked, the function reads the full document into a memory buffer and creates a hash table. The dictionary for a document should not exceed the available memory on the machine. This assumption is reasonable, since, a million-word dictionary with an average word length of ten bytes requires only 10 MB of memory.
                </p>
                <p>
                    The text_parser function uses Porter2 as the stemming algorithm.
                </p>
                <h4>Background</h4>
                <p>
                    General background on tokenization can be found here: http://en.wikipedia.org/wiki/ Lexical_analysis#Tokenizer
                </p>
                <p>
                    For more information about stemming, refer to: http://en.wikipedia.org/wiki/Stemming
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="TextChunker" style="display:none">
                <h2 class="text-center">TextChunker</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The text_chunker function divides text into phrases and assigns each phrase a tag identifying its type.             
                </p>
                <h4>Background</h4>
                <p>
                    Text chunking, also called shallow parsing, divides text into phrases in such a way that syntactically-related words become members of the same phrase. These phrases are non- overlapping, which means that one word can only be a member of one chunk. For example, the sentence “He reckons the current account deficit will narrow to only # 1.8 billion in September .” can be divided as follows:
                </p>
                <p>
                    [NP He ] [VP reckons ] [NP the current account deficit ] [VP will narrow ] [PP to ] [NP only # 1.8 billion ] [PP in ] [NP September ] .
                </p>
                <p>
                    In this example, chunks are represented as groups of words between square brackets. A tag next to the open bracket denotes the type of the chunk. This tag represents the phrase type
                </p>
                <p>
                    For more information about text chunking, refer to the following papers:
                </p>
                <p>
                    Erik F. Tjong Kim Sang and Sabine Buchholz, Introduction to the CoNLL-2000 Shared Task: Chunking. In: Proceedings of CoNLL-2000 and LLL-2000, Lisbon, Portugal, 2000.<br>
                    Fei Sha and Fernando Pereira, Shallow Parsing with Conditional Random Fields. [2003]
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="TextMorph" style="display:none">
                <h2 class="text-center">TextMorph</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    This function provides what is sometimes called lemmatization, a basic tool in text analysis. The TextMorph function outputs a standard form of the input words.             
                </p>
                <p>
                    When processing words:
                </p>
                <ul>
                    <li>If the input word is in the dictionary:</li>
                    <ul>
                        <li>If the word is not in the standard form, output the morph of the word and its corresponding Part of Speech (PoS).</li>
                        <li>If the word is in the original form, output the word and its corresponding PoS.</li>
                    </ul>
                    <li>If the input word is not in the dictionary, output the word and set its PoS to null.</li>
                <h4>Background</h4>
                <p>
                    The TextMorph function is a basic tool for text analysis. For example, in English, different forms of a word need to be morphed to the original form to improve the accuracy of text analysis.
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="TextTagging" style="display:none">
                <h2 class="text-center">TextTagging</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The TextTagging function tags input tuples according to user-defined rules. These rules comprise logical and text processing operators.            
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="TextTokenizer" style="display:none">
                <h2 class="text-center">TextTokenizer</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The TextTokenizer function extracts tokens (or text segments) from text. Examples of tokens are words, punctuation marks, and numbers.    
                </p>
                <h4>Backround</h4>
                <p>
                    Electronic text is a linear sequence of symbols (characters, words, or phrases). To perform certain types of text analysis, the first step is to tokenize the text. Tokenization is the process of extracting the linguistic units (such as words, punctuation marks, and numbers) from text being analyzed.
                </p>
                <p>
                    Tokenization of Chinese text presents challenges not found in other languages like English. In English, words are often separated by spaces, although there are cases like “Los Angeles” and “rock 'n' roll” where the spaces do not act as word delimiters.
                </p>
                <p>
                    In the first example, “Los Angeles” is an individual concept, even though it is made up from two words separated by a space. Similarly, “rock 'n' roll” is an individual concept made up of two words, two spaces, two punctuation marks, and a single character. In addition, there are cases where a punctuation mark serves as a separator of two words as in the case of the text “I’m”, which consists of “I” and “am.”
                </p>
                <p>
                    In Chinese, sentences are represented as strings of Chinese characters or hanzi where the space and apostrophe characters, which are natural word delimiters in English, are not used. This makes the extraction of tokens more challenging when processing Chinese text. The TextTokenizer function must first identify the sequence of words in a sentence and mark their boundaries.
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="Canopy" style="display:none">
                <h2 class="text-center">Canopy</h2>
                <div class="text_scroll">
                <h4>Introduction</h4>
                <p>
                    Canopy clustering is a very simple, fast, and surprisingly accurate method for grouping objects into preliminary clusters. Each object is represented as a point in a multidimensional feature space.           
                </p>
                <p>
                    The algorithm uses a fast approximate distance metric and two distance thresholds, T1 &gt; T2, for processing. The basic algorithm begins with a set of points and identifies each point with one or more canopies – groups of interrelated, "close", or "similar" points. Any point can belong to more than one canopy (so long as the distance from the canopy center to the point is &lt; T1), and thus judicious selection of canopy centers (with none being less than T2 apart from the next) and the points in a canopy allow for more efficient execution of clustering algorithms, which are often called within canopies.
                </p>
                <p>
                    Canopy clustering is often used as an initial step in more rigorous clustering techniques, such as k-means clustering. By starting with an initial partitioning into canopies, the number of more expensive distance measurements can be significantly reduced by ignoring points outside of the initial canopies.
                </p>
                <p>
                    Also, after the initial step divides points into their respective canopies, the second step need only perform intra-canopy clustering, which can be parallelized. In other words, points that do not belong to the same canopy do not have to be considered at the same time in the clustering process.
                </p>
                </div>
            </div>
            <div id="KMeans" style="display:none">
                <h2 class="text-center">KMeans</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    K-means is one of the simplest unsupervised learning algorithms that solve the well-known clustering problem. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters) fixed a priori. The main idea is to define k centroids, one for each cluster. This algorithm aims at minimizing an objective function, in this case a squared error function. The objective function, which is a chosen distance measure between a data point and the cluster center, is an indicator of the distance of the n data points from their respective cluster centers.            
                </p>
                <p>
                    The algorithm is composed of the following steps:
                </p>
                <ol>
                    <li>Place k points into the space represented by the objects that are being clustered. These points represent initial group centroids.</li>
                    <li>Assign each object to the group that has the closest centroid.</li>
                    <li>When all objects have been assigned, recalculate the positions of the k centroids.</li>
                    <li>Repeat steps 2 and 3 until the centroids no longer move. This produces a separation of the objects into groups from which the metric to be minimized can be calculated.</li>
                </ol>
                <p>
                    Although it can be proved that the procedure will always terminate, the k-means algorithm does not necessarily find the most optimal configuration, corresponding to the global objective function minimum. The algorithm is also significantly sensitive to the initial randomly selected cluster centers. The k-means algorithm can be run multiple times to reduce this effect.
                </p>
                <h4>Background</h4>
                <p>
                    The k-means algorithm in map-reduce consists of an iteration (until convergence) of a map and a reduce step. The map step assigns each point to a cluster. The reduce step takes all the points in each cluster and calculates the new centroid of the cluster.
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="KMeansPlot" style="display:none">
                <h2 class="text-center">KMeansPlot</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    After using the KMeans function to obtain the centroids (train the model), you can use the model to cluster new data points to these cluster centroids. The KMeansPlot function enables you to do that.           
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="Minhash" style="display:none">
                <h2 class="text-center">Minhash</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    Association analysis, clustering, and the detection of similarity between items using various metrics are frequently required in data analysis, particularly over large transactional data sets.   
                </p>
                <p>
                    Clustering algorithms such as the k-means algorithm and canopy partitioning perform well with physical data, but grouping items based on transaction history often requires less restrictive forms of analysis. Locality-sensitive hashing, commonly known as “minhash,” is a particularly effective way of grouping items together based on a Jaccard metric of similarity.
                </p>
                <p>
                    For example, we can declare two items to be similar because they are frequently placed in the same shopping basket by customers.
                </p>
                <p>
                    Following this approach, we can use minhash to analyze transaction data and identify clusters of “similar” items frequently bought together in a transaction. Alternatively, we might analyze the same transaction data and generate clusters of “similar” users based on the items they bought.
                </p>
                <h4>Backround</h4>
                <p>
                    Minhash is a probabilistic clustering method that assigns a pair of users to the same cluster with probability proportional to the overlap between the set of items that these users have bought (this relationship between users and items mimics various other transactional models). Each user u (who is a member of set U) is represented by a set of items that he has bought. The similarity between two users ui and uj is defined as the overlap between their item sets, given by the intersection of the item sets divided by the union of the item sets – commonly known as the “Jaccard coefficient” or “Jaccard metric.”
                </p>
                <p>
                    This similarity measure admits a locality-sensitive hashing scheme called minhash, which calculates one or more IDs for each user as the hash value (s) of a randomly chosen item from a permutation of the set of items that the user has bought. The probability that two users will be hashed to the same ID is exactly equal to their Jaccard coefficient S, as long as a class of universal hashing functions is used. To take this hashing scheme one step further, concatenating p hash-values (multiple hash values would be generated by hashing a random item from the item set with multiple hash functions) together as a distinct ID for each user makes the probability that any two users will agree on this concatenated hash key equivalent to Sp.
                </p>
                <p>
                    If each user is assigned to several ids, the odds of a collision with another id of a similar user increase. Thus, the minhash algorithm uses several hash functions, hashes a “randomly selected item” from the item set of each user (in this case the item that produces the minimum hash value for a particular hash function, hence the name of the algorithm) with each one of them, and concatenates groups of p hash values together to produce an ID, providing several ids for each user. Hence the number of key groups (p) must be a divisor of the total number of hash functions. Collisions between cluster ids lead to effective clustering.
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="Naive Bayes Functions" style="display:none">
                <h2 class="text-center">What is Naive Bayes?</h2>
                <div class="text_scroll">
                <p>
                    This is a set of functions to train a Naive Bayes classification model. The Naive Bayes algorithm is very simple, yet surprisingly effective. A training data set (for which we know discrete outcomes and either discrete or continuous input variables) is used to generate the model. The model is then used to predict the outcome of future observations, based on their input variables.            
                </p>
                <p>
                    There are two main components to the Naive Bayes model:
                </p>
                <ul>
                    <li>Bayes’ Theorem<br>
                        Bayes’ theorem is a classical law, stating that the probability of observing an outcome given the data is proportional to the probability of observing the data given the outcome, times the prior probability of the outcome.</li>
                    <li>The “naive” probability model<br>
                        The naive probability model is the assumption that the input data are independent of one another, and conditional on the outcome. This is a very strong assumption, and never true in real life, but it makes computation of all model parameters extremely simple, and violating the assumption does not hurt the model much.</li>
                </ul>
                </div>
            </div>
            <div id="Random Forest Functions" style="display:none">
                <h2 class="text-center">Random Forest Functions</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    SQL-MR provides a suite of functions to create a predictive model based on a combination of the Classification and Regression Trees (CART) algorithm for training decision trees, and the ensemble learning method of bagging.           
                </p>
                <p>
                    The SQL-MR decision tree functions are:
                </p>
                <ul>
                    <li>Forest_Drive—Builds a predictive model based on training data.</li>
                    <li>Forest_Predict—Uses the model generated by the Forest_Drive function to analyze the input data and make predictions.</li>
                    <li>Forest_Analyze—Analyzes the model generated by the Forest_Drive function and gives weight to the variables used in the model. This helps you understand the basis by which the Forest_Predict function makes predictions.</li>
                <h4>Background</h4>
                <p>
                    Decision trees are a supervised learning technique used for both classification and regression problems. In technical terms, a decision tree creates a piecewise constant approximation function for the training data.
                </p>
                <p>
                    Decision trees are a common procedure used in data mining and supervised learning because of their robustness to many of the problems of real world data, such as missing values, irrelevant variables, outliers in input variables, and variable scalings. The decision tree algorithm is an off-the-shelf procedure, with few parameters to tune.
                </p>
                <p>
                    The SQL-MR decision tree functions implement an algorithm for decision tree training and prediction based on Classification and Regression Trees by Breiman, Friedman, Olshen and Stone (1984).
                </p>
                </div>
            </div>
            <div id="Single Decision Tree Functions" style="display:none">
                <h2 class="text-center">Single Decision Tree Functions</h2>
                <div class="text_scroll">
                <p>
                    Decision trees are a common procedure used in data mining and supervised learning because of their robustness to many of the problems of real world data, such as missing values, irrelevant variables, outliers in input variables, and variable scalings. The algorithm is an “off- the-shelf ” procedure, with a few parameters to tune.          
                </p>
                <p>
                    This implementation creates only one decision tree as opposed to generating multiple tree in the case of random forests. This function at this point only supports classification trees on continuous variables, handles missing values during the prediction phase, supports GINI, entropy, and chi-square impurity measurements.
                </p>
                </div>
            </div>
            <div id="Basket_Generator" style="display:none">
                <h2 class="text-center">Basket_Generator</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    This function generates sets (“baskets”) of items. The input is typically a set of purchase transaction records or web pageview logs. Each basket is a unique combination or permutation of items. You specify the desired basket size. Combinations and Permutations are returned in lexicographical order.         
                </p>
                <p>
                    The resulting baskets can be used as part of a collaborative filtering algorithm. This is useful for analyzing purchase behavior of users in-store or on a website. This function can also operate on activity data (for example, “users who viewed this page also viewed this page”).
                </p>
                <h4>Background</h4>
                <p>
                    Retailers mine transaction data to track purchasing behavior or viewing behavior. A retailer's goal is to find interesting combinations (called baskets) of items purchased together or shopped for at the same time. A frequent need is to automatically identify interesting baskets and also look for trends over time and compare other attributes (for example, compare stores). Having a general function that can operate on data structured in the form often present for retails will make interesting market basket analysis possible.
                </p>
                <p>
                    This general function is intended to help facilitate market basket analysis by operating on data that is structured in a form typical of retail transaction history databases.
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="CFilter" style="display:none">
                <h2 class="text-center">CFilter</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    This function performs collaborative filtering via a series of SQL commands and SQL-MapReduce functions. You run this function via an internal JDBC wrapper function.     
                </p>
                <h4>Background</h4>
                <p>
                    Collaborative filtering is used by analysts to find items or events that are frequently paired with other items or events. For example, the Amazon.com feature, “People who shopped for this item also shopped for...” uses a collaborative filtering algorithm. Another use is “People who viewed this profile also viewed this profile” on LinkedIn. Aster Database’s collaborative filter (cfilter) is a general-purpose tool that can provide answers in many similar-use cases.
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="WSRecommender" style="display:none">
                <h2 class="text-center">WSRecommender</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    WSRecommender is an item-based, collaborative filtering function that uses a weighted-sum algorithm to make recommendations (for example, items or products that users should consider purchasing).     
                </p>
                <h4>Background</h4>
                <p>
                    A recommendation (or recommender) system is an information filtering system that predicts the ratings or preferences that users assign to entities like books, cars, movies, songs, and people. Recommender systems have become very common lately. For example, online retailers use recommender systems to recommend additional products shoppers can add to their carts.
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="AllPairsShortestPath" style="display:none">
                <h2 class="text-center">AllPairsShortestPath</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    This function computes the shortest distances between all combinations of the specified source and target vertices. The function works on directed and undirected graphs. The function also works on unweighted (uniform edges having weights of length one) and weighted graphs (non-negative edge weights).     
                </p>
                <h4>Background</h4>
                <p>
                    The all-pairs shortest path algorithm determines the shortest graph distances between every pair of vertices in a graph. It has wide applicability in graph analysis. For example:
                </p>
                <ul>
                    <li>The resulting pairs and distances can be aggregated to determine a closeness metric for each vertex in a graph. Closeness measures the average of the shortest distances from a given vertex to all others. Closeness is a fundamental metric used by social network analysis applications.</li>
                    <li>The resulting pairs and distances can be aggregated to determine the k-degree for each vertex in a graph. The k-degree of a vertex measures the number of vertices that are within distance k from a given vertex. K-degree also has applications in social network analysis.</li>
                </ul>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="Betweenness" style="display:none">
                <h2 class="text-center">Betweenness</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The Betweenness function determines betweenness, a type of centrality measurement, for every vertex (also referred to as node) in the input graph.   
                </p>
                <h4>Background</h4>
                <p>
                    Betweenness is a measure of the centrality of a node in a network, and is calculated as the fraction of shortest paths between node pairs that pass through the node of interest.
                </p>
                <p>
                    Betweenness is, in some sense, a measure of the influence a node has over the spread of information through the network.
                </p>
                <p>
                    By counting only shortest paths, however, the conventional definition implicitly assumes that information spreads only along those shortest paths.
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="Closeness" style="display:none">
                <h2 class="text-center">Closeness</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The Closeness function computes closeness and k-degree scores for each specified source vertex in a graph. The function works on directed and undirected graphs. The function also works on unweighted and weighted graphs (non-negative edge weights).     
                </p>
                <p>
                    The Closeness function provides the inverse of the sum, the inverse of the average, and the sum of inverses for the shortest distances to all reachable target vertices as different closeness metrics.
                </p>
                <p>
                    An efficient approximation of the closeness and k-degree scores can be computed for large graphs by executing the function on a random sample of the specified target vertices.
                </p>
                <h4>Background</h4>
                <p>
                    Closeness and k-degree are fundamental distance-based centrality metrics used in network structure analysis. Both can be regarded as a measure of how long it takes to spread information from a given vertex v to a set of target vertices.
                </p>
                <p>
                    Closeness is classically defined for each vertex v as either the inverse sum or the inverse average of the shortest path distances from v to all other reachable vertices u.
                </p>
                <p>
                    The classical definition of closeness is not applicable to disconnected graphs. However, there have been alternative closeness metrics proposed for such graphs. The Closeness function computes the alternative closeness score proposed for disconnected graphs. This alternative definition adds 0 to the sums for each unreachable target vertex, which is consistent with the classic definition as the inverse distance is effectively 0 in this case.
                </p>
                <p>
                    K-degree is defined for a vertex v as the number of vertices whose distance from v is less than or equal to K.
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="EigenvectorCentrality" style="display:none">
                <h2 class="text-center">EigenvectorCentrality</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The EigenvectorCentrality function calculates the centrality (relative importance) of each node in a graph.   
                </p>
                <p>
                    This function implements some of the common Eigenvector Centrality algorithms, including Katz centrality and Bonacich Centrality.
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="LocalClusteringCoefficient" style="display:none">
                <h2 class="text-center">LocalClusteringCoefficient</h2>
                <div class="text_scroll">
                <h4>Summary</h4>
                <p>
                    The clustering coefficient is a frequently-used tool for analyzing the structure of a network. It was introduced in the context of binary undirected graphs. But this function extends it to directed and weighted graphs as well.   
                </p>
                <h4>Usage</h4>
                </div>
            </div>
            <div id="LoopyBeliefPropagation" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="nTree" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="PageRank" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="Antiselect" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="Apache Log Parser" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="IdentityMatch" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="IpGeo" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="JSONParser" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="Multicase" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="MurmurHash" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="OutlierFilter" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="PSTParserAFS" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="Pack" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="Pivot" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="Unpack" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="Unpivot" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="XMLParser" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="XMLRelation" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="CfilterViz" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
            <div id="NpathViz" style="display:none">
                <script src="https://gist.github.com/PMeinshausen/ffeac15ed5f289a8d387.js"></script>
            </div>
        </div>
    </div>

<script>

    //var names_list = ["Attribution", "CMAVG", "DTW", "DWT", "DWT2D", "FrequentPaths", "IDWT", "IDWT2D", "Path_Analyzer", "Path_Generator", "Path_Start", "Path_Summarizer", "SAX", "Sessionization", "nPath", "Approximate Distinct Count", "Approximate Percentile", "ConfusionMatrix", "Correlation", "Distribution Matching", "EMAVG", "Enhanced Histogram Function", "FMeasure", "GLM", "GLMPredict", "Histogram", "KNN", "LARS Functions", "LinReg", "Percentile", "Principal Component Analysis", "Sample", "SMAVG", "Support Vector Machines", "VWAP", "WMAVG", "LDA Functions", "Levenshtein Distance", "Naive Bayes Text Classifier", "Named Entity Recognition", "nGram", "PoSTagger", "Sentenizer", "Sentiment Extraction Functions", "TF_IDF", "Text_Classifier", "Text_Parser", "TextChunker", "TextMorph", "TextTagging", "TextTokenizer", "Canopy", "KMeans", "KMeansPlot", "Minhash", "Naive Bayes Functions", "Random Forest Functions", "Single Decision Tree Functions", "Basket_Generator", "CFilter", "WSRecommender", "AllPairsShortestPath", "Betweenness", "Closeness", "EigenvectorCentrality", "LocalClusteringCoefficient", "LoopyBeliefPropagation", "nTree", "PageRank", "Antiselect", "Apache Log Parser", "IdentityMatch", "IpGeo", "JSONParser", "Multicase", "MurmurHash", "OutlierFilter", "PSTParserAFS", "Pack", "Pivot", "Unpack", "Unpivot", "XMLParser", "XMLRelation", "CfilterViz", "NpathViz"]    

    var margin = {top: 20, right: 80, bottom: 20, left: 120},
        width = 680 - margin.right - margin.left,
        height = 900 - margin.top - margin.bottom;

    var i = 0,
        duration = 750,
        root;

    var tree = d3.layout.tree()
        .size([height, width]);

    var diagonal = d3.svg.diagonal()
        .projection(function(d) { return [d.y, d.x]; });

    var svg = d3.select("div#chart").append("svg")
        .attr("width", width + margin.right + margin.left)
        .attr("height", height + margin.top + margin.bottom)
      .append("g")
        .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

    
    d3.json("aster_functions.json", function(error, flare){
        thisroot = flare;

        new_root = JSON.parse(JSON.stringify(thisroot));

      names = [];
      for (i=0; i < new_root['children'].length; i++){
          for (j=0; j < new_root['children'][i]['children'].length; j++){
              names.push(new_root['children'][i]['children'][j].name);
          }
      };
      names_list = JSON.parse(JSON.stringify(names));
      console.log(names_list);
    })
    

    d3.json("aster_functions.json", function(error, flare) {
      root = flare;
      root.x0 = height / 2;
      root.y0 = 0;

      function collapse(d) {
        if (d.children) {
          d._children = d.children;
          d._children.forEach(collapse);
          d.children = null;
        }
      }

      root.children.forEach(collapse);
      update(root);
    });

    d3.select(self.frameElement).style("height", "900px");

    function update(source) {

      var nodes = tree.nodes(root).reverse(),
          links = tree.links(nodes);

      nodes.forEach(function(d) { d.y = d.depth * 180; });

      var node = svg.selectAll("g.node")
          .data(nodes, function(d) { return d.id || (d.id = ++i); });

      var nodeEnter = node.enter().append("g")
          .attr("class", "node")
          .attr("transform", function(d) { return "translate(" + source.y0 + "," + source.x0 + ")"; })
          .on("click", function(d){ 
            if ($.inArray(d.name, names_list) < 0){
                click(d); update(d); }
            else {
                update_info(d);
            }
            });

      nodeEnter.append("circle")
          .attr("r", 1e-6)
          .style("fill", function(d) { return d._children ? "orange" : "#FF6E00"; });

      nodeEnter.append("text")
          .attr("x", function(d) { return d.children || d._children ? -10 : 10; })
          .attr("dy", ".35em")
          .attr("text-anchor", function(d) { return d.children || d._children ? "end" : "start"; })
          .text(function(d) { return d.name; })
          .style("fill-opacity", 1e-6)
          .on('mouseover', function(){
            d3.select(this).style({'text-decoration': 'underline'});
          })
          .on('mouseout', function(){
            d3.select(this).style({'text-decoration': 'none'});
          });
          /*
          .on('click', function(d){ 
            if ($.inArray(d.name, names_list) > -1){
                update_info(d);
                }
          });
*/

      /*
      d3.selectAll('.node_text')
          .on('click', function(d){
            console.log("this");
            update_info(d);
      });
      */

    

      var nodeUpdate = node.transition()
          .duration(duration)
          .attr("transform", function(d) { return "translate(" + d.y + "," + d.x + ")"; });

      nodeUpdate.select("circle")
          .attr("r", 6.0)
          .style("fill", function(d) { return d._children ? "orange" : "#FF6E00"; });

      nodeUpdate.select("text")
          .style("fill-opacity", 1);

      var nodeExit = node.exit().transition()
          .duration(duration)
          .attr("transform", function(d) { return "translate(" + source.y + "," + source.x + ")"; })
          .remove();

      nodeExit.select("circle")
          .attr("r", 1e-6);

      nodeExit.select("text")
          .style("fill-opacity", 1e-6);

      var link = svg.selectAll("path.link")
          .data(links, function(d) { return d.target.id; });

      link.enter().insert("path", "g")
          .attr("class", "link")
          .attr("d", function(d) {
            var o = {x: source.x0, y: source.y0};
            return diagonal({source: o, target: o});
          });

      link.transition()
          .duration(duration)
          .attr("d", diagonal);

      link.exit().transition()
          .duration(duration)
          .attr("d", function(d) {
            var o = {x: source.x, y: source.y};
            return diagonal({source: o, target: o});
          })
          .remove();

      nodes.forEach(function(d) {
        d.x0 = d.x;
        d.y0 = d.y;
      });
    }


    function click(d) {

      if (d.children) {
        d._children = d.children;
        d.children = null;
      } else {
        d.children = d._children;
        d._children = null;
      }
      //update(d);
      //update_info(d);
    };

    function update_info(d){
        
        for (i=0; i<names_list.length; i++){
            var the_id = names_list[i];
            if (document.getElementById(the_id).style.display === "" || document.getElementById(the_id).style.display == "block"){
                $(document.getElementById(the_id)).hide();
            }
        };

        
        $(document.getElementById(d.name)).show();
        //$("div[id ="+d.name+"]").show();

    };

</script>



</body>
